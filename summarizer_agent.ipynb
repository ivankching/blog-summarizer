{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e01b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.genai import types\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fec8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_config=types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504], # Retry on these HTTP errors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8e643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_md_file(tool_context: ToolContext, filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the content of a specified text file.\n",
    "\n",
    "    Args:\n",
    "        tool_context: The ADK tool context.\n",
    "        filename: The name of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the file content.\n",
    "    \"\"\"\n",
    "    file_path = Path(filepath)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return {\"status\": \"success\", \"content\": content}\n",
    "    except FileNotFoundError:\n",
    "        return {\"status\": \"error\", \"message\": f\"File not found: {filepath}\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c585dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = LlmAgent(\n",
    "    name=\"summarizer\",\n",
    "    model=Gemini(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        retry_options=retry_config\n",
    "    ),\n",
    "    instruction=\"Your primary function is to read files using the provided tool and summarize their content.\",\n",
    "    tools=[read_md_file]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4313d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Created new session: debug_session_id\n",
      "\n",
      "User > Read the file from filepath post_content/Ahead of AI/beyond-standard-llms.md and summarize its content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarizer > The file discusses various alternative architectures to standard autoregressive transformer Large Language Models (LLMs).\n",
      "\n",
      "Here's a summary of the key alternatives:\n",
      "\n",
      "1.  **Linear Attention Hybrids:** These models aim to improve efficiency by using linear attention mechanisms instead of the standard quadratic attention, which can be computationally expensive for long sequences. Examples include Qwen3-Next and Kimi Linear, which combine linear attention (like Gated DeltaNet) with standard attention layers in a hybrid approach. While they offer efficiency gains (reduced FLOPs and KV memory), they may involve added complexity and a slight trade-off in accuracy.\n",
      "\n",
      "2.  **Text Diffusion Models:** Inspired by diffusion models used in image generation, these models offer a different approach to text generation. Their main advantage is the potential for parallel token generation, which could be more efficient than the sequential generation of autoregressive LLMs. However, they face challenges with output quality degradation, inability to stream answers, and potential difficulties with tool-calling and chain-of-thought reasoning. Models like LLaDA are examples.\n",
      "\n",
      "3.  **Code World Models (CWM):** This approach focuses on improving modeling performance, particularly for code generation. Instead of just predicting the next token, CWM learns to simulate the execution of code by predicting program states. This aims to provide a deeper \"understanding of the world\" (in this case, the world of code execution). The Code World Models paper introduced a 32B parameter model that shows promise in improving code understanding and reasoning.\n",
      "\n",
      "4.  **Small Recursive Transformers:** These are highly specialized, small transformer models designed for reasoning tasks. Examples like the Hierarchical Reasoning Model (HRM) and Tiny Recursive Model (TRM) demonstrate impressive reasoning capabilities on specific benchmarks (like ARC and Sudoku) through iterative self-refinement. While not general-purpose LLMs, they show the potential for efficient, task-specific reasoning modules.\n",
      "\n",
      "In conclusion, while standard autoregressive transformer LLMs remain the go-to for many tasks due to their proven capabilities and mature tooling, these alternative approaches offer exciting directions for improving efficiency, exploring new generation paradigms, enhancing code understanding, and achieving specialized reasoning abilities.\n"
     ]
    }
   ],
   "source": [
    "runner = InMemoryRunner(agent=summarizer)\n",
    "response = await runner.run_debug(\n",
    "    \"Read the file from filepath post_content/Ahead of AI/beyond-standard-llms.md and summarize its content\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a17a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
